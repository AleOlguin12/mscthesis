---
title: "NN-example"
author: "Andres A Saenz Guzman"
date: "19/5/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, results='hide'}
library(ggplot2)
library(tidyverse)
library(tensorflow)
library(keras)
library(forecast)
library(caret)
library(plyr)
library(dplyr)
library(reticulate)

```

Here I present an example to introduce neural networks (NN) to the project. In order to achieve this I create a non-linear time series using the Sin function. 

```{r}
my_fake_time_series <- as.ts(sin(1:1000/100) + sin(1:1000/10))

plot(my_fake_time_series)
```

Lets start by using the nnetar() function of the 'Forecast' package. First I divide the data into training and testing. I use 90% of data for training and 10% for testing.

```{r}
n <- nrow(as.data.frame(my_fake_time_series))
train_fake <- as.ts(my_fake_time_series[1:round(n*0.9, digits = 0)])
test_fake <- as.ts(my_fake_time_series[(round(n*0.9, digits = 0) + 1):n])

```

Once the data is splitted, I can build the model with nnetar.

```{r nnetar}
lags <- 50
neurons <- 6 #Number of nodes in the hidden layer
sims <- 400
L <- 1 #Univariate data
N <- nrow(as.data.frame(train_fake))



nnetar.fake <- nnetar(y = train_fake, 
                    p = lags, 
                    size = neurons,
                    repeats = sims,
                    )

 
fcast.nnetar.fake <- forecast(nnetar.fake, 
                  h = 100, 
                  PI = TRUE, 
                  level = c(80,95), 
                  fan = TRUE,,
                  npaths = 40
                  )
         
autoplot(my_fake_time_series, series = 'Data', ) +
  autolayer(fcast.nnetar.fake, series = 'nnetar',)

```

The performance seems to be good, but how does it compare to an AR(1) process?

```{r}
fake.ar1 <- Arima(train_fake, order = c(1,0,0))

fcast.ar1.fake <- forecast(fake.ar1, h = 100)
autoplot(my_fake_time_series, series = 'Data') +
  
  autolayer(fcast.ar1.fake, series = 'AR(1)') +
  autolayer(fcast.nnetar.fake, series = 'nnetar')
```

nnetar's single layer perceptron clearly outperforms the AR(1) process. This is due to the high non-linearity of the generated data. However, it may be interesting to compare its performance with an RNN with the 'keras' package adapted for R, since the complexity of the code required to produce output with 'keras' is at a different scale.


```{r libraries, results='hide'}
library(tibble)
library(keras)
library(tensorflow)
```

lets start with data preprocessing

```{r preprocessing}
fake_data <- data.matrix(my_fake_time_series)

mean <- apply(fake_data, 2, mean) #2 indicates columns, 1 indicates rows
std <- apply(fake_data, 2, sd)
fake_data <- scale(fake_data, center = mean, scale = std)


norm <- function(x){
  return((x - min(x)) / (max(x) - min(x)))
}

max <- apply(fake_data, 2, max)
min <- apply(fake_data, 2, min)

fake_data <- apply(fake_data, 2, norm)

autoplot(as.ts(fake_data))
```

generator function for training, validation and testing data without storing the data.

```{r data generator function}
generator <- function(data,
                      lookback,
                      delay,
                      min_index,
                      max_index,
                      shuffle = FALSE,
                      batch_size = 128,
                      step = 6) {
  if (is.null(max_index))
    max_index <- nrow(data) - delay - 1
  i <- min_index + lookback
  function() {
    if (shuffle) {
      rows <-
        sample(c((min_index + lookback):max_index), size = batch_size)
    } else {
      if (i + batch_size >= max_index)
        i <<- min_index + lookback
      rows <- c(i:min(i + batch_size - 1, max_index))
      i <<- i + length(rows)
    }
    samples <- array(0, dim = c(length(rows),
                                lookback / step,
                                dim(data)[[-1]]))
    targets <- array(0, dim = c(length(rows)))
    
    for (j in 1:length(rows)) {
      indices <- seq(rows[[j]] - lookback, rows[[j]] - 1,
                     length.out = dim(samples)[[2]])
      samples[j, , ] <- data[indices, ]
      targets[[j]] <- data[rows[[j]] + delay, 1]
    }
    list(samples, targets)
  }
}



```

```{r flatten NN}
lookback <- 10 #lags
step <- 1 
delay <- 100
batch_size <- 20
n <- round(nrow(as.data.frame(my_fake_time_series)), digits = 0)*0.9 
  
train_gen <- generator(
  fake_data,
  lookback = lookback,
  delay = delay,
  min_index = 1,
  max_index = n,
  shuffle = FALSE,
  step = step,
  batch_size = batch_size)

train_gen_data <- train_gen()

model <- keras_model_sequential() %>% 
  layer_flatten(input_shape = c(lookback / step, dim(fake_data)[-1])) %>% 
  layer_dense(units = 40, activation = 'relu') %>% 
  layer_dense(units = 20, activation = 'relu') %>% 
  layer_dense(units = 1)

summary(model)

model %>% compile(optimizer_rmsprop(),
                  loss = "mae")

history <- model %>% fit_generator(train_gen,
                                   steps_per_epoch = 250,
                                   epochs = 10,)

plot(history)
```

```{r}
batch_size_plot <- 91
lookback_plot <- lookback
step_plot <- 1

pred_gen <- generator(
  fake_data,
  lookback = lookback_plot,
  delay = 0,
  min_index = n,
  max_index = nrow(fake_data),
  shuffle = FALSE,
  step = step_plot,
  batch_size = batch_size_plot)

pred_gen_data <- pred_gen()

v1 <- seq(1, length(pred_gen_data[[2]]))

plot_data <- as.data.frame(cbind(v1, pred_gen_data[[2]]))

input_data <- pred_gen_data[[1]]
dim(input_data) <- c(batch_size_plot, lookback, 1) # c(size, lags, predictors)

pred_out <- model %>% 
  predict(input_data)

plot_data <- cbind(plot_data, pred_out)

p <- ggplot(plot_data, aes(x=v1, y=V2)) + 
  geom_point(color="blue", size=0.1, alpha=0.4)

p <- p + geom_point(aes(x=v1, y=pred_out), color="red", size=0.1, alpha=0.4)

p
```

```{r generating array}
T_data <- fake_data

x1 <- data.frame()
for (i in 1:nrow(T_data)) {
  x1 <- rbind(x1, t(rev(T_data[i:(i + 50)])))
  if(i%%100 == 0){print(i)}
}

x1 <- x1[,order(ncol(x1):1)]

x <- as.matrix(x1[,-51])
y <- as.matrix(x1[, 51])

dim(x) <- c(nrow(T_data), 50, 1)

```

Now we train the model

```{r}

model <- keras_model_sequential() %>% 
  layer_lstm(units = 64, input_shape = c(50, 1)) %>% 
  layer_dense(units = 64, activation = "relu") %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 8, activation = "relu") %>% 
  layer_dense(units = 1, activation = "relu")

summary(model)

model %>% compile(loss = "mean_squared_error", optimizer = "adam", metrics = "mse")

history <- model %>% fit(x,
                         y,
                         batch_size=100,
                         epochs = 10,
                         validation_split = 0.1,
                         use_multiprocessing = TRUE)

plot(history)

```


Now I plot the results

```{r}
batch_size_plot <- 51
lookback_plot <- 50
step_plot <- 1

pred_gen <- generator(
  fake_data,
  lookback = lookback_plot,
  delay = 0,
  min_index = n,
  max_index = nrow(fake_data),
  shuffle = FALSE,
  step = step_plot,
  batch_size = batch_size_plot
)

pred_gen_data <- pred_gen()

v1 = seq(1, length(pred_gen_data[[2]]))

plot_data <- as.data.frame(cbind(v1, pred_gen_data[[2]]))

input_data <- pred_gen_data[[1]][, , 1]
dim(input_data) <- c(batch_size_plot, lookback_plot, 1)

pred_out <- model %>% predict(input_data)

p <- ggplot(plot_data, aes(x=v1, y=V2)) + 
  geom_point(color="blue", size=0.1, alpha=0.4)

p <- p + geom_point(aes(x=v1, y=pred_out), color="red", size=0.1, alpha=0.4)

p
```

Now I can do a real RNN


```{r}
fake_data2 <- fake_data - 0.5 #centers around zero
plot(fake_data2)
```


```{r}


batch <- 50
lookback <- 50
step <- 1
delay <- 50

train_gen <- generator(
  fake_data2,
  lookback = lookback,
  delay = delay,
  min_index = 1,
  max_index = 500,
  shuffle = FALSE,
  step = step,
  batch_size = batch_size
)

val_gen <- generator(
  fake_data2,
  lookback = lookback,
  delay = delay,
  min_index = 501,
  max_index = 950,
  shuffle = FALSE,
  step = step,
  batch_size = batch_size
)

model <- keras_model_sequential() %>% 
  layer_gru(units = 64, return_sequences = TRUE, input_shape = c(NULL, 50, 1)) %>%
  bidirectional(layer_gru(units = 64)) %>% 
  layer_dense(units = 64, activation = "tanh") %>% 
  layer_dense(units = 32, activation = "tanh") %>% 
  layer_dense(units = 1, activation = "tanh")



model %>% compile(optimizer = optimizer_rmsprop(), loss = "mse")

summary(model)

callbacks = callback_early_stopping(monitor = "val_loss", min_delta = 0,
                                    patience = 10, verbose = 0, mode = "auto",
                                    baseline = NULL, restore_best_weights = TRUE)

history <- model %>% fit_generator(train_gen,
                                   steps_per_epoch = 35,
                                   epochs = 50,
                                   callbacks = callbacks,
                                   validation_data = val_gen,
                                   validation_steps = 5)

plot(history)
```

```{r}
batch_size_plot <- 950
lookback_plot <- 50
step_plot <- 1

pred_gen <- generator(
  fake_data2,
  lookback = lookback_plot,
  delay = 0,
  min_index = 1,
  max_index = nrow(fake_data),
  shuffle = FALSE,
  step = step_plot,
  batch_size = batch_size_plot
)

pred_gen_data <- pred_gen()

v1 = seq(1, length(pred_gen_data[[2]]))

plot_data <- as.data.frame(cbind(v1, pred_gen_data[[2]]))

input_data <- pred_gen_data[[1]][, , 1]
dim(input_data) <- c(batch_size_plot, lookback_plot, 1)

pred_out <- model %>% predict(input_data)

p <- ggplot(plot_data, aes(x=v1, y=V2)) + 
  geom_point(color="blue", size=0.1, alpha=0.4)

p <- p + geom_point(aes(x=v1, y=pred_out), color="red", size=0.1, alpha=0.4)

p
```

```{r}
test_gen <- generator(
  fake_data2,
  lookback = lookback,
  delay = delay,
  min_index = 900,
  max_index = NULL,
  step = step,
  batch_size = batch_size,
  )

test_gen_data <- test_gen

future_values <- model %>% predict(test_gen_data, steps = 25)
plot(future_values)
p + geom_point(aes(x=v1, y=future_values), color="green", size=0.1, alpha=0.4)

```












